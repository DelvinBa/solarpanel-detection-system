# Automatic Solar Panel Detection

## üìå Project Background

The energy transition is not just a technical challenge but a societal one. The **NOWATT** project aims to involve residents, SMEs, and government entities in accelerating energy neutrality at the neighborhood level using **Artificial Intelligence (AI)**. One critical aspect is **predicting the energy labels** of homes in the Netherlands.

A major factor influencing a home's energy efficiency is **solar panel installation**. By **detecting solar panels on rooftops**, we can improve **energy label prediction models** and help housing associations meet sustainability requirements by **2030**. Organizations like **Nijhuis Bouw** are actively working on these improvements.

This project leverages **computer vision** to detect solar panels using satellite imagery, forming a crucial step in energy label classification.

---

## üõ†Ô∏è Project Scope & Goals

In this project, we focus on:
- üì° **Satellite Image Processing** ‚Äì Detect solar panels from aerial/satellite images.
- üè† **Data Integration** ‚Äì Align detected solar panels with **open-source** data like **Kadaster**.
- ü§ñ **Object Detection Algorithms** ‚Äì Experiment with **YOLO, Faster R-CNN, SSD**, etc.
- üîÑ **Automated ML Pipeline** ‚Äì Develop a **data pipeline** that automates preprocessing, model training, and evaluation.
- üöÄ **API Deployment** ‚Äì Convert the trained model into an **API** that accepts images and returns solar panel predictions.


## Prerequisites

Ensure you have the following installed:

- [Docker](https://docs.docker.com/get-docker/)
- [Docker Compose](https://docs.docker.com/compose/install/)
- [Python 3.10+](https://www.python.org/downloads/)
- [pip](https://pip.pypa.io/en/stable/)

## ‚öôÔ∏è Setup Guide

### **1Ô∏è‚É£ Clone the Repository**
```bash
git clone <repository-url>
cd AutomaticSolarPanelDetection 
```

### **2Ô∏è‚É£ Create a Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate     # Windows
```

### **3Ô∏è‚É£ Install Dependencies**
```bash
pip install -r requirements.txt
```

### **4Ô∏è‚É£ Configure Environment Variables**
The project uses environment variables for configuration. You can either:

1. Use the default values in the `.env` file
2. Create your own `.env` file based on the provided template

```bash
# Copy the template and modify as needed
cp .env.template .env
```

### **5Ô∏è‚É£ Prepare YOLO Model**
Ensure your YOLO model is correctly placed in the appropriate directory:

```bash
# Create directory if it doesn't exist
mkdir -p airflow/dags/models

# Place your YOLO model file (e.g., best5.pt) in this directory
# cp /path/to/your/model.pt airflow/dags/models/best5.pt
```

### **6Ô∏è‚É£ Start the Services**
```bash
docker-compose up -d
```

This command will start all necessary services:
- **MLflow** server on port 5000
- **Airflow** webserver on port 8080
- **MinIO** server on port 9000 (API) and 9001 (Console)
- **PostgreSQL** databases for both MLflow and Airflow

### **7Ô∏è‚É£ Verify Services**

- **MLflow UI**: http://localhost:5000
- **Airflow UI**: http://localhost:8080 (Username: admin, Password: admin)
- **MinIO Console**: http://localhost:9001 (Username: minioadmin, Password: minioadmin)
- **pgAdmin**: http://localhost:5050 (Email: admin@admin.com, Password: admin)

### **8Ô∏è‚É£ Accessing and Managing Databases with pgAdmin**

Here's how to add and check your databases in pgAdmin:

1. Open your browser and go to http://localhost:5050
2. Login with:
   - Email: admin@admin.com
   - Password: admin
3. Right-click on "Servers" in the left panel and select "Create" > "Server"
4. In the "General" tab, give it a name like "Local PostgreSQL"
5. In the "Connection" tab, enter:
   - Host: postgres
   - Port: 5432
   - Maintenance database: airflow
   - Username: airflow
   - Password: airflow
6. Click "Save" to connect to your PostgreSQL server
7. You should now see both the `airflow` and `mlflow` databases in the server tree

### **9Ô∏è‚É£ Set Up MinIO Bucket**

1. Open the MinIO Console: http://localhost:9001
2. Log in using the credentials (default: minioadmin/minioadmin)
3. Create a bucket called `mybucket` for storing images
4. Upload your images to this bucket

## üöÄ Usage

### Training YOLO Models with MLflow Integration

We've integrated YOLO model training with MLflow for experiment tracking and model management. This allows you to:

1. **Track Experiments**: Log parameters, metrics, and artifacts during training
2. **Compare Models**: Easily compare different model configurations
3. **Version Control**: Register and version trained models
4. **Load for Inference**: Seamlessly load models for inference

#### Training a YOLO Model

To train a YOLO model with MLflow tracking:

```bash
# Basic training with default parameters
python src/traintest/train_yolo.py

# Custom training with specific parameters
python src/traintest/train_yolo.py --model yolov8m.pt --epochs 50 --batch 16 --img_size 832
```

#### Running Inference with MLflow-tracked Models

```bash
# Using the latest model from the registry
python src/traintest/predict_mlflow.py --image path/to/image.jpg

# Using a specific MLflow run
python src/traintest/predict_mlflow.py --image path/to/image.jpg --run_id <mlflow_run_id>
```

For more details on the MLflow integration, see the [MLflow YOLO Integration Documentation](docs/mlflow_yolo_integration.qmd).

### Running the Airflow DAG

The YOLO detection pipeline runs automatically every 5 minutes through an Airflow DAG. The DAG:

1. Fetches images from the MinIO bucket
2. Processes them with the YOLO model
3. Saves the labeled images back to MinIO in the "labeled-images" folder
4. Logs detection results to the PostgreSQL database

You can manually trigger the DAG from the Airflow UI:

1. Open http://localhost:8080
2. Navigate to the DAGs page
3. Find the "yolo_minio_airflow" DAG
4. Click the "Trigger DAG" button

### Tracking Model Performance with MLflow

MLflow can be used to track model performance, versions, and experiments:

1. Open http://localhost:5000
2. View experiments, runs, and metrics
3. Compare different model versions

## üìÅ Project Organization

```
‚îú‚îÄ‚îÄ .env                    <- Environment variables for local development
‚îú‚îÄ‚îÄ config.env              <- Environment variables for Docker containers
‚îú‚îÄ‚îÄ docker-compose.yml      <- Docker Compose configuration
‚îú‚îÄ‚îÄ airflow/                <- Airflow DAGs and configuration
‚îÇ   ‚îî‚îÄ‚îÄ dags/               <- Airflow DAG definitions
‚îÇ       ‚îú‚îÄ‚îÄ models/         <- YOLO model files
‚îÇ       ‚îî‚îÄ‚îÄ process_minio_yolo.py <- Main DAG for image processing
‚îú‚îÄ‚îÄ mlflow/                 <- MLflow server configuration
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile          <- Dockerfile for MLflow server
‚îú‚îÄ‚îÄ data/                   <- Data files
‚îÇ   ‚îú‚îÄ‚îÄ external/           <- Data from third party sources
‚îÇ   ‚îú‚îÄ‚îÄ interim/            <- Intermediate processed data
‚îÇ   ‚îú‚îÄ‚îÄ processed/          <- Final processed data for modeling
‚îÇ   ‚îî‚îÄ‚îÄ raw/                <- Original, immutable data
‚îú‚îÄ‚îÄ models/                 <- Trained and serialized models
‚îú‚îÄ‚îÄ notebooks/              <- Jupyter notebooks for exploration
‚îú‚îÄ‚îÄ src/traintest/          <- YOLO model training and testing
‚îÇ   ‚îú‚îÄ‚îÄ train_yolo.py       <- MLflow-integrated YOLO training script
‚îÇ   ‚îú‚îÄ‚îÄ predict_mlflow.py   <- Prediction script using MLflow models
‚îÇ   ‚îú‚îÄ‚îÄ data.yaml           <- YOLO dataset configuration
‚îÇ   ‚îî‚îÄ‚îÄ yolo_SolarPanel.ipynb <- Original YOLO training notebook
‚îú‚îÄ‚îÄ src/                    <- Source code
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         <- Makes src a Python module
‚îÇ   ‚îú‚îÄ‚îÄ config.py           <- Configuration settings
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py          <- Dataset handling
‚îÇ   ‚îú‚îÄ‚îÄ features.py         <- Feature engineering
‚îÇ   ‚îî‚îÄ‚îÄ modeling/           <- Model training and prediction
‚îú‚îÄ‚îÄ docs/                   <- Documentation
‚îÇ   ‚îî‚îÄ‚îÄ mlflow_yolo_integration.qmd <- MLflow integration documentation
‚îú‚îÄ‚îÄ requirements.txt        <- Python dependencies
‚îî‚îÄ‚îÄ README.md               <- Project documentation
```

## üõ†Ô∏è Configuration Options

### Environment Variables

Key environment variables that can be configured:

| Variable | Description | Default |
|----------|-------------|---------|
| PG_USER | PostgreSQL username for MLflow | mlflow |
| PG_PASSWORD | PostgreSQL password for MLflow | mlflow |
| PG_DATABASE | PostgreSQL database for MLflow | mlflow |
| MLFLOW_PORT | Port for MLflow server | 5000 |
| MLFLOW_BUCKET_NAME | MinIO bucket for MLflow artifacts | mlflow |
| MINIO_ROOT_USER | MinIO root username | minioadmin |
| MINIO_ROOT_PASSWORD | MinIO root password | minioadmin |
| MINIO_PORT | MinIO API port | 9000 |
| MINIO_CONSOLE_PORT | MinIO Console port | 9001 |


## Accessing the Services

- Airflow Web Interface: http://localhost:8080
  - Username: admin (default)
  - Password: admin (default)
- MLflow UI: http://localhost:5000
- MinIO Console: http://localhost:9001
  - Username: minioadmin (default)
  - Password: minioadmin (default)


