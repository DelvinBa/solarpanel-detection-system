# Automatic Solar Panel Detection

## ğŸ“Œ Project Background

The energy transition is not just a technical challenge but a societal one. The **NOWATT** project aims to involve residents, SMEs, and government entities in accelerating energy neutrality at the neighborhood level using **Artificial Intelligence (AI)**. One critical aspect is **predicting the energy labels** of homes in the Netherlands.

A major factor influencing a home's energy efficiency is **solar panel installation**. By **detecting solar panels on rooftops**, we can improve **energy label prediction models** and help housing associations meet sustainability requirements by **2030**. Organizations like **Nijhuis Bouw** are actively working on these improvements.

This project leverages **computer vision** to detect solar panels using satellite imagery, forming a crucial step in energy label classification.

---

## ğŸ› ï¸ Project Scope & Goals

In this project, we focus on:
- ğŸ“¡ **Satellite Image Processing** â€“ Detect solar panels from aerial/satellite images.
- ğŸ  **Data Integration** â€“ Align detected solar panels with **open-source** data like **Kadaster**.
- ğŸ¤– **Object Detection Algorithms** â€“ Experiment with **YOLO, Faster R-CNN, SSD**, etc.
- ğŸ”„ **Automated ML Pipeline** â€“ Develop a **data pipeline** that automates preprocessing, model training, and evaluation.
- ğŸš€ **API Deployment** â€“ Convert the trained model into an **API** that accepts images and returns solar panel predictions.

You will work with:
âœ… **Raw & annotated satellite image data**  
âœ… **Preprocessing scripts & model training code**  
âœ… **Object detection models (YOLO, etc.)**  

---

## ğŸ—ï¸ Architecture

This project consists of several components:

1. **MLflow** - For model tracking, versioning, and serving
2. **Airflow** - For automated workflow orchestration
3. **MinIO** - For object storage (images and model artifacts)
4. **PostgreSQL** - For metadata storage
5. **YOLO** - For object detection

### System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚     â”‚             â”‚     â”‚             â”‚
â”‚   Images    â”‚â”€â”€â”€â”€â–¶â”‚   Airflow   â”‚â”€â”€â”€â”€â–¶â”‚  Database   â”‚
â”‚  (MinIO)    â”‚     â”‚  Workflow   â”‚     â”‚ (PostgreSQL)â”‚
â”‚             â”‚     â”‚             â”‚     â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                           
                           â–¼                           
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    
                    â”‚             â”‚                    
                    â”‚  YOLO Model â”‚                    
                    â”‚             â”‚                    
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    
                           â”‚                           
                           â–¼                           
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             â”‚     â”‚             â”‚     â”‚             â”‚
â”‚  MLflow     â”‚â—€â”€â”€â”€â”€â”‚  Processed  â”‚â”€â”€â”€â”€â–¶â”‚  Labeled    â”‚
â”‚  Tracking   â”‚     â”‚   Results   â”‚     â”‚   Images    â”‚
â”‚             â”‚     â”‚             â”‚     â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Prerequisites

Ensure you have the following installed:

- [Docker](https://docs.docker.com/get-docker/)
- [Docker Compose](https://docs.docker.com/compose/install/)
- [Python 3.10+](https://www.python.org/downloads/)
- [pip](https://pip.pypa.io/en/stable/)

## âš™ï¸ Setup Guide

### **1ï¸âƒ£ Clone the Repository**
```bash
git clone <repository-url>
cd AutomaticSolarPanelDetection
```

### **2ï¸âƒ£ Create a Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate     # Windows
```

### **3ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

### **4ï¸âƒ£ Configure Environment Variables**
The project uses environment variables for configuration. You can either:

1. Use the default values in the `.env` file
2. Create your own `.env` file based on the provided template

```bash
# Copy the template and modify as needed
cp .env.template .env
```

### **5ï¸âƒ£ Prepare YOLO Model**
Ensure your YOLO model is correctly placed in the appropriate directory:

```bash
# Create directory if it doesn't exist
mkdir -p airflow/dags/models

# Place your YOLO model file (e.g., best5.pt) in this directory
# cp /path/to/your/model.pt airflow/dags/models/best5.pt
```

### **6ï¸âƒ£ Start the Services**
```bash
docker-compose up -d
```

This command will start all necessary services:
- **MLflow** server on port 5000
- **Airflow** webserver on port 8080
- **MinIO** server on port 9000 (API) and 9001 (Console)
- **PostgreSQL** databases for both MLflow and Airflow

### **7ï¸âƒ£ Verify Services**

- **MLflow UI**: http://localhost:5000
- **Airflow UI**: http://localhost:8080 (Username: admin, Password: admin)
- **MinIO Console**: http://localhost:9001 (Username: minioadmin, Password: minioadmin)

### **8ï¸âƒ£ Set Up MinIO Bucket**

1. Open the MinIO Console: http://localhost:9001
2. Log in using the credentials (default: minioadmin/minioadmin)
3. Create a bucket called `mybucket` for storing images
4. Upload your images to this bucket

## ğŸš€ Usage

### Training YOLO Models with MLflow Integration

We've integrated YOLO model training with MLflow for experiment tracking and model management. This allows you to:

1. **Track Experiments**: Log parameters, metrics, and artifacts during training
2. **Compare Models**: Easily compare different model configurations
3. **Version Control**: Register and version trained models
4. **Load for Inference**: Seamlessly load models for inference

#### Training a YOLO Model

To train a YOLO model with MLflow tracking:

```bash
# Basic training with default parameters
python traintest/train_yolo.py

# Custom training with specific parameters
python traintest/train_yolo.py --model yolov8m.pt --epochs 50 --batch 16 --img_size 832
```

#### Running Inference with MLflow-tracked Models

```bash
# Using the latest model from the registry
python traintest/predict_mlflow.py --image path/to/image.jpg

# Using a specific MLflow run
python traintest/predict_mlflow.py --image path/to/image.jpg --run_id <mlflow_run_id>
```

For more details on the MLflow integration, see the [MLflow YOLO Integration Documentation](docs/mlflow_yolo_integration.qmd).

### Running the Airflow DAG

The YOLO detection pipeline runs automatically every 5 minutes through an Airflow DAG. The DAG:

1. Fetches images from the MinIO bucket
2. Processes them with the YOLO model
3. Saves the labeled images back to MinIO in the "labeled-images" folder
4. Logs detection results to the PostgreSQL database

You can manually trigger the DAG from the Airflow UI:

1. Open http://localhost:8080
2. Navigate to the DAGs page
3. Find the "yolo_minio_airflow" DAG
4. Click the "Trigger DAG" button

### Tracking Model Performance with MLflow

MLflow can be used to track model performance, versions, and experiments:

1. Open http://localhost:5000
2. View experiments, runs, and metrics
3. Compare different model versions

## ğŸ“ Project Organization

```
â”œâ”€â”€ .env                    <- Environment variables for local development
â”œâ”€â”€ config.env              <- Environment variables for Docker containers
â”œâ”€â”€ docker-compose.yml      <- Docker Compose configuration
â”œâ”€â”€ airflow/                <- Airflow DAGs and configuration
â”‚   â””â”€â”€ dags/               <- Airflow DAG definitions
â”‚       â”œâ”€â”€ models/         <- YOLO model files
â”‚       â””â”€â”€ process_minio_yolo.py <- Main DAG for image processing
â”œâ”€â”€ mlflow/                 <- MLflow server configuration
â”‚   â””â”€â”€ Dockerfile          <- Dockerfile for MLflow server
â”œâ”€â”€ data/                   <- Data files
â”‚   â”œâ”€â”€ external/           <- Data from third party sources
â”‚   â”œâ”€â”€ interim/            <- Intermediate processed data
â”‚   â”œâ”€â”€ processed/          <- Final processed data for modeling
â”‚   â””â”€â”€ raw/                <- Original, immutable data
â”œâ”€â”€ models/                 <- Trained and serialized models
â”œâ”€â”€ notebooks/              <- Jupyter notebooks for exploration
â”œâ”€â”€ traintest/              <- YOLO model training and testing
â”‚   â”œâ”€â”€ train_yolo.py       <- MLflow-integrated YOLO training script
â”‚   â”œâ”€â”€ predict_mlflow.py   <- Prediction script using MLflow models
â”‚   â”œâ”€â”€ data.yaml           <- YOLO dataset configuration
â”‚   â””â”€â”€ yolo_SolarPanel.ipynb <- Original YOLO training notebook
â”œâ”€â”€ src/                    <- Source code
â”‚   â”œâ”€â”€ __init__.py         <- Makes src a Python module
â”‚   â”œâ”€â”€ config.py           <- Configuration settings
â”‚   â”œâ”€â”€ dataset.py          <- Dataset handling
â”‚   â”œâ”€â”€ features.py         <- Feature engineering
â”‚   â””â”€â”€ modeling/           <- Model training and prediction
â”œâ”€â”€ docs/                   <- Documentation
â”‚   â””â”€â”€ mlflow_yolo_integration.qmd <- MLflow integration documentation
â”œâ”€â”€ requirements.txt        <- Python dependencies
â””â”€â”€ README.md               <- Project documentation
```

## ğŸ› ï¸ Configuration Options

### Environment Variables

Key environment variables that can be configured:

| Variable | Description | Default |
|----------|-------------|---------|
| PG_USER | PostgreSQL username for MLflow | mlflow |
| PG_PASSWORD | PostgreSQL password for MLflow | mlflow |
| PG_DATABASE | PostgreSQL database for MLflow | mlflow |
| MLFLOW_PORT | Port for MLflow server | 5000 |
| MLFLOW_BUCKET_NAME | MinIO bucket for MLflow artifacts | mlflow |
| MINIO_ROOT_USER | MinIO root username | minioadmin |
| MINIO_ROOT_PASSWORD | MinIO root password | minioadmin |
| MINIO_PORT | MinIO API port | 9000 |
| MINIO_CONSOLE_PORT | MinIO Console port | 9001 |

## ğŸ§¹ Cleanup

To stop and remove all containers:

```bash
docker-compose down
```

To also remove volumes (warning: this will delete all data):

```bash
docker-compose down -v
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the terms of the [LICENSE](LICENSE) file included in the repository.

## Platform-Specific Setup

### Windows
1. Install Docker Desktop for Windows
2. Make sure WSL2 is enabled (required for Docker Desktop)
3. Clone this repository
4. Open PowerShell or Command Prompt in the project directory
5. Run:
   ```powershell
   docker-compose up -d
   ```

### macOS
1. Install Docker Desktop for Mac
2. Clone this repository
3. Open Terminal in the project directory
4. Run:
   ```bash
   docker-compose up -d
   ```

### Linux
1. Install Docker and Docker Compose
2. Clone this repository
3. Open Terminal in the project directory
4. Run:
   ```bash
   docker-compose up -d
   ```

## Accessing the Services

- Airflow Web Interface: http://localhost:8080
  - Username: admin (default)
  - Password: admin (default)
- MLflow UI: http://localhost:5000
- MinIO Console: http://localhost:9001
  - Username: minioadmin (default)
  - Password: minioadmin (default)

## Project Structure

```
.
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/              # Airflow DAG files
â”‚   â”œâ”€â”€ Dockerfile         # Custom Airflow image
â”‚   â””â”€â”€ start-airflow.sh   # Airflow startup script
â”œâ”€â”€ mlflow/                # MLflow configuration
â”œâ”€â”€ docker-compose.yml     # Docker Compose configuration
â””â”€â”€ .env                   # Environment variables (create from .env.example)
```

## Troubleshooting

If you encounter any issues:

1. Make sure all required ports are available
2. Check if Docker is running properly
3. Try cleaning up Docker resources:
   ```bash
   docker-compose down -v
   docker system prune -f
   ```
4. Restart Docker Desktop (Windows/macOS) or Docker service (Linux)
5. Check the logs:
   ```bash
   docker-compose logs -f
   ```

## Development

To modify the project:

1. Edit the DAG files in `airflow/dags/`
2. Rebuild the containers:
   ```bash
   docker-compose build
   docker-compose up -d
   ```

## License

[Your License Here]

