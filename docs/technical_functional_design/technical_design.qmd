---
title: "Technical Design"
author:
  - Navid Gharapanjeh
  - Delvin Bacho
date: "2025-02-07"
toc: true
format:
    html: 
        code-fold: true
    
---


# 1. Introduction 
This document is about the technical part of this solar detection project. What this project is about and what the goals and requirements are, is described extensively in the  [functional design](functional_design.qmd).
The technical design is documented using the arc42 template and will tailor it for this project if needed. The architectural diagrams should be in c4/mermaid.


# 2. Constraints
- **Technical constraints**: 
  - Hardware, software and cloud providers should be free or open source
  - Cloud providers: Either Cloud Student Accounts (e.g. Azure or AWS) or Saxions AWS Account
  - Our own machine is not ideal, since we train on images (limited computing power if we have no computation server)
- **Operational constraints**: 
  - Deadline: 20. April 2025
  - Personell: Group of two software engineers/computer scientists, with limited knowledge in DataScience/ML
  - We have the Training data given by the project, which is publicly available. However the Inference data, is not available, and needs to be scraped by us. Training Data (houses of south germany) and Inference data (houses of netherlands), will therefore be not of the same format and region.

# 3. System Scope and Context


This section provides an **overview of the system landscape** by illustrating **who interacts with our system and how it fits into its environment**. It consists of two context diagrams:

1. **Solar Panel Detection System**  
   - Shows the primary users and external systems that interact with our Solar Panel Detection System.


## 3.1 Context Diagrams

### Solar Panel Detection System - Context Diagram  
![Context Diagram - Solarpanel Detection System](images/c4_context_diagram.svg)

The **Solar Panel Detection System** is responsible for **analyzing house images to detect solar panels**. It includes a lightweight internal data ingestion process (implemented via simple Python scripts) that fetches aerial images of Dutch houses from public services.

The main stakeholders and external systems involved are:

**Users:**
- **Developers**: Upload training images and manage the retraining process.
- **Nijhuis Bouw (Client)**: Uploads inference images and checks detection results.
- **Selin (Product Owner)**: Also uploads inference images and verifies detection results.

**External Systems:**
- **CommonDataFactory**: External service that receives a **city name** and returns a **list of addresses**.
- **Bag Viewer Kadaster**: Receives an **address** and returns **X, Y coordinates**.
- **PDOK Luchtfoto WMS**: Receives **X, Y coordinates** and returns the corresponding **aerial image** of the house.
- **Data Storage for Results**: This is an existing Excel file from Selin, where detection results are stored alongside other project-related data (e.g., for energy label calculation). A migration to a proper database is being considered.




# 4. Solution Strategy

The **main goal** of this project is to build a **fully automated, monolithic Data and ML Pipeline** that detects solar panels in aerial or satellite images and links these detections to Dutch house IDs. We adopt a **simplified machine learning pipeline**—as illustrated below—focusing on the following stages:

1. **Data Ingestion / Versioning**  
2. **Data Preprocessing**  
3. **Model Training**  
4. **Model Deployment**  
5. **Model Validation**  
6. **Model Feedback**

In general we need to distinguish between multiple big processes we will build here. 

1. Training Pipeline
This process is mostly about the above image/pipeline.
2. Dutch houses scraping process
This is what we called Webscraping System in chapter 3 System Scope and Context. This could be also seen as a part of Data ingestion 
3. Inference Pipeline

> **Note:** The pipeline diagram is adapted from the *DataOps Specialization (2024-2025), Deployment and Operations* lecture materials by Deepak Tunuguntla and Pieter Zeilstra (Saxion University of Applied Sciences).  
> For this project, **Data Validation**, **Model Tuning** and **Model Analysis** are **out of scope**.

![ML Pipeline](images/ml-pipeline.png)

Because our resources are limited and the team is small, a **monolithic end-to-end pipeline** is simpler to maintain than multiple microservices. We will still keep each phase (Ingestion, Preprocessing, Training, etc.) **modular** within the monolithic structure, so we can evolve individual steps if needed.

- **Automation & Reproducibility**: We plan to automate pipeline execution using a scheduling or orchestration mechanism (e.g., Apache Airflow).  
- **Accuracy & Domain Adaptation**: We will compare multiple Machine Learning object detection models (e.g., YOLO, Faster R-CNN). Although model tuning is out of scope, we aim to ensure a robust baseline accuracy given our training data (Germany) differs from inference data (Netherlands).  
- **House ID Linking**: A subsystem (still part of the monolith) will handle address lookups via Kadaster/PDOK APIs, linking detection results to the specific records in the existing data storage from the Selin, where more information about the same house are stored.

---

## 4.1 Addressing Key Business Goals

| **Business Goal**                                     | **Scenario**                                                                                                                                                                 | **Solution Approach**                                                                                                                                                 |
|--------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Goal 1: Automation of Solarpanel Detection System**  | - New aerial/satellite images should trigger the pipeline automatically. <br/> - Minimal manual intervention once data arrives.                                             | - **Monolithic end-to-end pipeline** with automated scheduling/orchestration. <br/> - **Containerized steps** (if applicable) to ensure consistent environments. <br/> - **Version control** for reproducibility. |
| **Goal 2: Solar Panel Detection Accuracy**             | - Model must reliably detect panels in Dutch aerial imagery. <br/> - Compare different object detection frameworks for baseline accuracy.                                    | - **Evaluate multiple ML models** (e.g., YOLO, Faster R-CNN).                |
| **Goal 3: House ID Detection / Kadaster Integration**  | - After detecting solar panels, link results to specific addresses in the final data storage                             | <br/> - **Coordinate-based matching** of bounding boxes to house polygons. <br/> - **Automated data retrieval** via public APIs and Webservices.                |




## 4.2 Technology Selection and Rationale


## Overview  
In this project, we are building an **object detection pipeline** using **YOLOv8** to analyze satellite images of houses and detect solar panels. The pipeline is designed to be **scalable, automated, and efficient**, leveraging **Apache Airflow** for orchestration, **MinIO** as the storage backend, and **PostgreSQL** as the database. Since the focus is on model training and inference, **ETL/ELT and data cleaning are not yet included**, but these may be added in the future for data transformation and preprocessing.  

## Workflow Structure  
- **MinIO** acts as an **object storage system** to store raw images and model outputs.  
- **Apache Airflow** is used to **orchestrate** the workflow, triggering model training and inference jobs.  
- **MLflow** is used for experiment **tracking, model versioning**, and logging performance metrics to ensure reproducibility and model management.
- **YOLOv8** is the chosen object detection model for identifying solar panels on roofs.  
- **PostgreSQL** is the database for storing metadata, job statuses, and model performance metrics.  

Each of these components was carefully selected based on **scalability, ease of integration, and performance**. The table below provides a comparison of the chosen tools and their alternatives.  


### Selected Components and Alternatives Comparison

| **Component**       | **Chosen Tool**   | **Why Chosen?**                                                                                                                                                    | **Alternative(s)**        | **Why Not Chosen?**                                                                                                                                  |
|--------------------|-----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|
| **Storage**       | **MinIO**        | - S3-compatible API, making migration and integration easier  <br> - Lightweight and can be self-hosted on-premises or in the cloud  <br> - Scalable and performant for large datasets  <br> - Open-source and cost-effective compared to managed solutions | AWS S3, Azure Blob, GCS | - Managed cloud services can be expensive for large-scale storage  <br> - Vendor lock-in concerns  <br> - Less flexibility in self-hosted environments |
| **Orchestration** | **Apache Airflow** | - Industry-standard tool for workflow orchestration  <br> - Scalable, supports distributed execution  <br> - Large community and extensive plugin support  <br> - Integrates well with MinIO, Python, and ML pipelines | Prefect, Dagster, Kubeflow | - **Prefect** is easier to use but lacks some enterprise-level capabilities like complex DAGs  <br> - **Dagster** is great for data pipelines but less mature for ML workflows  <br> - **Kubeflow** is powerful but complex to set up and maintain |
| **Model Training** | **YOLOv8**       | - State-of-the-art real-time object detection  <br> - Pre-trained models available, reducing training time  <br> - Optimized for edge and cloud deployment  <br> - Faster inference compared to alternatives, making it suitable for real-time applications | Detectron2, MMDetection, EfficientDet | - **Detectron2** allows more customization but has a steeper learning curve  <br> - **MMDetection** is powerful but requires extensive configuration  <br> - **EfficientDet** provides better accuracy but is slower for real-time object detection |
| **Database** | **PostgreSQL** | - Perfect integration with Airflow (Airflow's native metadata DB)  <br> - PostGIS extension for geospatial data (crucial for property location analysis)  <br> - Strong data integrity with ACID compliance  <br> - Open-source with no licensing costs  <br> - JSON support for flexible schema when needed  <br> - Scalable for both logging and analytical queries | SQL Server, MongoDB, TimescaleDB | - **SQL Server** has licensing costs and higher resource requirements  <br> - **MongoDB** has weaker integration with Airflow and less mature spatial capabilities  <br> - **TimescaleDB** is specialized for time-series data which is only one aspect of our needs |





# 5. Building Block View


# 6. Runtime View
- **Data flow**: How data moves through the system.
- **Model serving process**: How predictions are generated and served.
- **Monitoring & logging**: Performance tracking and debugging.

# 7. Deployment View
- **Infrastructure choices**: Cloud services, local servers.
- **CI/CD pipeline for ML models**.
- **Containerization strategy**: Docker, Kubernetes.
- **Versioning & rollback mechanisms**.

# 8. Cross-cutting Concepts

# 9. Architectural Decisions

# 10. Quality Requirements


# 11. Risks and Technical Debt
- **Potential biases in data and models**.
- **Computational cost and resource constraints**.
- **Integration challenges with existing systems**.


---

